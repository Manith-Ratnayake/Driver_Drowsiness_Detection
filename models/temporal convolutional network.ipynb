{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOY_CPPTreqc",
        "outputId": "81829ddd-e87b-4c55-ea60-bdf93a2f8944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/manith04/ddd-processed-1-training-frames-type-1?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.14G/4.14G [03:12<00:00, 23.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/manith04/ddd-processed-2-training-frames-type-1?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.37G/3.37G [02:38<00:00, 22.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (1.0.0).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/manith04/ddd-processed-validation-frames-type-1?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.57G/1.57G [01:12<00:00, 23.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"manith04/ddd-processed-1-training-frames-type-1\")\n",
        "path = kagglehub.dataset_download(\"manith04/ddd-processed-2-training-frames-type-1\")\n",
        "path = kagglehub.dataset_download(\"manith04/ddd-processed-validation-frames-type-1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyhNPXh9tZIv",
        "outputId": "6d2f00fc-a6ed-4b8a-ae7f-b6a24eb38182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "004  022  026  030\n"
          ]
        }
      ],
      "source": [
        "!ls  /root/.cache/kagglehub/datasets/manith04/ddd-processed-validation-frames-type-1/versions/1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VlHCvQa5e45",
        "outputId": "174c8798-2bb6-4dd8-eb82-86fcc2d6314c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 85ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install \"Pillow<10\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ToUlV-ptA_z",
        "outputId": "82e7c38a-f56c-42ea-d6d7-2d783e07be92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "001  002  005  006  008  009  012  013\t015  020\n"
          ]
        }
      ],
      "source": [
        "!ls /root/.cache/kagglehub/datasets/manith04/ddd-processed-1-training-frames-type-1/versions/1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHWas20quPiZ",
        "outputId": "d32076fe-aaf7-45c8-a3fa-558848665996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 92ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 29ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 84ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==9.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.1.1\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !uv pip uninstall pillow torchvision\n",
        "# !uv pip  install torch==2.2.0 torchvision==0.15.2 torchaudio==2.2.0 \"Pillow<10\"\n",
        "# !uv  pip install \"Pillow<10\" --force-reinstall\n",
        "!uv pip install torchvision\n",
        "!uv pip install -U pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-EH9wwNrb1s"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, UnidentifiedImageError\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.transforms import v2 as T\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import ConcatDataset\n",
        "\n",
        "\n",
        "\n",
        "class DrowsySequenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, seq_len=15, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Root directory containing subject folders.\n",
        "            seq_len (int): Number of consecutive frames per sequence.\n",
        "            transform (callable, optional): Optional transform to apply to each frame.\n",
        "        \"\"\"\n",
        "        self.samples = []\n",
        "        self.seq_len = seq_len\n",
        "        self.transform = transform\n",
        "\n",
        "        # Iterate over subjects\n",
        "        for subject in os.listdir(root_dir):\n",
        "            subject_path = os.path.join(root_dir, subject)\n",
        "            if not os.path.isdir(subject_path):\n",
        "                continue\n",
        "\n",
        "            # Iterate over scenarios\n",
        "            for scenario in os.listdir(subject_path):\n",
        "                scenario_path = os.path.join(subject_path, scenario)\n",
        "                if not os.path.isdir(scenario_path):\n",
        "                    continue\n",
        "\n",
        "                # Iterate over videos (or frames directly under scenario)\n",
        "                for video in os.listdir(scenario_path):\n",
        "                    video_path = os.path.join(scenario_path, video)\n",
        "                    if not os.path.isdir(video_path):\n",
        "                        video_path = scenario_path  # Handle case frames are directly under scenario\n",
        "\n",
        "                    frame_files = sorted([\n",
        "                        f for f in os.listdir(video_path)\n",
        "                        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "                    ])\n",
        "\n",
        "                    # Skip videos shorter than seq_len\n",
        "                    if len(frame_files) < seq_len:\n",
        "                        continue\n",
        "\n",
        "                    # Create sequences\n",
        "                    for i in range(len(frame_files) - seq_len + 1):\n",
        "                        self.samples.append((video_path, frame_files[i:i + seq_len]))\n",
        "\n",
        "        print(f\"[Dataset] Total sequences found: {len(self.samples)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_dir, frame_list = self.samples[idx]\n",
        "        images = []\n",
        "\n",
        "        for fname in frame_list:\n",
        "            img_path = os.path.join(video_dir, fname)\n",
        "            try:\n",
        "                img = Image.open(img_path)  # Pillow ≤9 works fine\n",
        "                if img.mode != \"L\":\n",
        "                    img = img.convert(\"L\")  # convert to grayscale\n",
        "\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "\n",
        "                # Ensure [C, H, W]\n",
        "                if isinstance(img, torch.Tensor) and img.ndim == 2:\n",
        "                    img = img.unsqueeze(0)\n",
        "\n",
        "                images.append(img)\n",
        "\n",
        "            except (UnidentifiedImageError, OSError) as e:\n",
        "                print(f\"[Warning] Could not load {img_path}: {e}\")\n",
        "                # Add a zero tensor placeholder\n",
        "                images.append(torch.zeros(1, 224, 224))\n",
        "\n",
        "        images = torch.stack(images)  # [seq_len, C, H, W]\n",
        "\n",
        "        # Label from last frame filename, e.g., \"001_glasses_nonsleepyCombination_000000_0.jpg\"\n",
        "        last_fname = frame_list[-1]\n",
        "        label = int(last_fname.rsplit(\"_\", 1)[-1].split(\".\")[0])\n",
        "\n",
        "        return images, torch.tensor(label, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZB2BPDArb1w"
      },
      "outputs": [],
      "source": [
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, feature_dim=512, pretrained=True):\n",
        "        super().__init__()\n",
        "        mobilenet = models.mobilenet_v3_small(pretrained=True)\n",
        "        old_weights = mobilenet.features[0][0].weight          # Change first conv to accept 1 channel (grayscale)\n",
        "\n",
        "        mobilenet.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mobilenet.features[0][0].weight[:] = old_weights.mean(dim=1, keepdim=True)          # Initialize weights by averaging RGB weights\n",
        "\n",
        "        self.features = mobilenet.features          # Remove classifier, keep convolution layers only\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))           # Global average pooling\n",
        "        self.fc = nn.Linear(1280, feature_dim)          # Project 1280 → feature_dim\n",
        "\n",
        "        for param in self.features.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B*T, 1, H, W)\n",
        "        x = self.features(x)        # (B*T, 1280, H', W')\n",
        "        x = self.pool(x)            # (B*T, 1280, 1, 1)\n",
        "        x = x.flatten(1)            # (B*T, 1280)\n",
        "        x = self.fc(x)              # (B*T, feature_dim)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CausalTemporalConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.padding = (kernel_size - 1) * dilation\n",
        "\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels, out_channels, kernel_size, padding=self.padding, dilation=dilation\n",
        "        )\n",
        "\n",
        "        self.norm = nn.GroupNorm(\n",
        "            num_groups=8, num_channels=out_channels\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.residual = (\n",
        "            nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, C, T)\n",
        "        out = self.conv(x)\n",
        "\n",
        "        # Remove future padding (causal trimming)\n",
        "        out = out[:, :, :-self.padding]\n",
        "\n",
        "        out = self.norm(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out + self.residual(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CNNTemporalConv(nn.Module):\n",
        "    def __init__(self, feature_dim=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # Frame encoder\n",
        "        self.encoder = CNNEncoder(feature_dim)\n",
        "\n",
        "        # Temporal network\n",
        "        self.tcn = nn.Sequential(\n",
        "            CausalTemporalConvBlock(feature_dim, 256, kernel_size=3, dilation=1),\n",
        "            CausalTemporalConvBlock(256, 256, kernel_size=3, dilation=2),\n",
        "            CausalTemporalConvBlock(256, 128, kernel_size=3, dilation=4),\n",
        "        )\n",
        "\n",
        "        # Final classifier (only last time step)\n",
        "        self.classifier = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, T, 3, H, W)\n",
        "        \"\"\"\n",
        "\n",
        "        B, T, C, H, W = x.shape\n",
        "\n",
        "        # Merge batch and time\n",
        "        x = x.view(B * T, C, H, W)\n",
        "\n",
        "        # Extract frame features\n",
        "        features = self.encoder(x)           # (B*T, F)\n",
        "\n",
        "        # Restore time dimension\n",
        "        features = features.view(B, T, -1)   # (B, T, F)\n",
        "\n",
        "        # Conv1d expects (B, C, T)\n",
        "        features = features.transpose(1, 2)  # (B, F, T)\n",
        "\n",
        "        # Temporal convolution\n",
        "        tcn_out = self.tcn(features)         # (B, 128, T)\n",
        "\n",
        "        # Take LAST time step\n",
        "        last_feature = tcn_out[:, :, -1]     # (B, 128)\n",
        "\n",
        "        logits = self.classifier(last_feature)  # (B, 1)\n",
        "\n",
        "        return logits.squeeze(1)  # (B,)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1z28bVprb1w"
      },
      "outputs": [],
      "source": [
        "\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.Resize((112, 112)),\n",
        "    T.ToImage(),                        # PIL → Tensor (still uint8)\n",
        "    T.ToDtype(torch.float32, scale=1/255),\n",
        "    T.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
        "])\n",
        "\n",
        "\n",
        "train_ds1 = DrowsySequenceDataset(\"/root/.cache/kagglehub/datasets/manith04/ddd-processed-1-training-frames-type-1/versions/1\", transform=transform)\n",
        "train_ds2 = DrowsySequenceDataset(\"/root/.cache/kagglehub/datasets/manith04/ddd-processed-2-training-frames-type-1/versions/1\", transform=transform)\n",
        "train_ds = ConcatDataset([train_ds1, train_ds2])\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "val_ds = DrowsySequenceDataset(\"/root/.cache/kagglehub/datasets/manith04/ddd-processed-validation-frames-type-1/versions/1\", transform=transform)\n",
        "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "#test_ds = DrowsySequenceDataset(\"/kaggle/input/\", transform=transform)\n",
        "#test_loader = DataLoader(val_ds, batch_size=4, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "model = CNNTemporalConv().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "epochs = 51\n",
        "best_val_loss = float(\"inf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJt8Ntc8rb1x"
      },
      "outputs": [],
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
        "\n",
        "    for frames, labels in progress_bar:\n",
        "        frames = frames.to(device)\n",
        "        labels = labels.float().to(device)\n",
        "\n",
        "        outputs = model(frames)\n",
        "        loss = criterion(outputs, labels.float())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Update progress bar with current loss\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for frames, labels in val_loader:\n",
        "            frames = frames.to(device)\n",
        "            labels = labels.float().to(device)\n",
        "\n",
        "            outputs = model(frames)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).int()\n",
        "            correct += (preds == labels.int()).sum().item()\n",
        "            total += labels.numel()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_acc = correct / total\n",
        "    print(f\"Epoch {epoch+1} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # Save last model\n",
        "    torch.save(model.state_dict(), \"last_model.pth\")\n",
        "\n",
        "    # Save best model\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")\n",
        "        print(f\"Epoch {epoch+1}: Best model saved!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f45QPt0Mrb1x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 9440749,
          "sourceId": 14769758,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 9440924,
          "sourceId": 14770017,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 9441838,
          "sourceId": 14771302,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31260,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}